import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score,
    recall_score, confusion_matrix
)
import string
import pickle
import warnings
warnings.filterwarnings('ignore')

# 1. Load Dataset
print("Loading dataset...")
df = pd.read_csv("spam.csv", encoding='ISO-8859-1')
df = df[[col for col in df.columns if 'Unnamed' not in str(col)]]
df.columns = ['label', 'text']

# 2. Preprocessing

# Offline-safe stopwords list
custom_stopwords = set([
    "the", "and", "is", "in", "to", "with", "a", "an", "on", "this", "that", "it",
    "as", "for", "of", "at", "by", "are", "from", "be", "have", "has", "or", "was",
    "i", "we", "you", "he", "she", "they", "but", "not", "so", "if", "do", "does",
    "can", "will", "just", "up", "out", "about", "all", "what", "when", "who", "how"
])

def clean_text(text):
    no_punc = ''.join([char for char in text if char not in string.punctuation])
    words = [word for word in no_punc.split() if word.lower() not in custom_stopwords]
    return ' '.join(words)

print("Cleaning text...")
df['clean_text'] = df['text'].astype(str).apply(clean_text)
df['label'] = df['label'].map({'ham': 0, 'spam': 1})

print("\nCleaned Text Samples:")
print(df[['text', 'clean_text']].head())

# 3. Feature Engineering
tfidf = TfidfVectorizer(max_features=5000)
X = tfidf.fit_transform(df['clean_text']).toarray()
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Model Training
models = {
    "Naive Bayes": MultinomialNB(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "SVM": SVC(kernel='linear', probability=True),
    "Random Forest": RandomForestClassifier()
}

results = {}
print("\nTraining models...")
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    results[name] = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred)
    }

# 5. Results
print("\nðŸ“Š Model Performance:")
for name, metrics in results.items():
    print(f"{name}: {metrics}")

# 6. Visualization
metrics_df = pd.DataFrame(results).T
metrics_df.plot(kind='bar', figsize=(10, 5))
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Confusion Matrix for Best Model (Logistic Regression)
best_model = LogisticRegression(max_iter=1000)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

plt.figure(figsize=(6,5))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d',
            cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])
plt.title('Confusion Matrix')
plt.tight_layout()
plt.show()

# 7. Save model & vectorizer
with open('spam_classifier.pkl', 'wb') as f:
    pickle.dump(best_model, f)
with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf, f)

# 8. Prediction Function
def predict_spam(email_text):
    cleaned = clean_text(email_text)
    vectorized = tfidf.transform([cleaned])
    prediction = best_model.predict(vectorized)[0]
    prob = best_model.predict_proba(vectorized)[0][1]
    return {
        'prediction': 'spam' if prediction == 1 else 'ham',
        'probability': round(prob, 4)
    }

# 9. Test Example
test_email = "WINNER!! As a valued network customer you have been selected to receive a prize!"
result = predict_spam(test_email)
print("\nTest Prediction:")
print(f"Text: {test_email}")
print(f"Prediction: {result['prediction']}, Probability: {result['probability']}")
